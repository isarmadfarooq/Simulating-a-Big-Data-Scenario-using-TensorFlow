# Simulating a Big Data Scenario using TensorFlow

This project generates a synthetic dataset with 1,000,000 rows and multiple features, preprocesses it, and trains a neural network using TensorFlow. The dataset is stored in **Parquet** format, and the model is saved in the **Keras format (`.keras`)**.

## ğŸ“Œ Project Structure
```
D:\Online Assigenment\MSC_DataAnalytic\Simulating-a-Big-Data-Scenario-using-TensorFlow
â”‚â”€â”€ train_data.parquet
â”‚â”€â”€ test_data.parquet
â”‚â”€â”€ best_model.keras
â”‚â”€â”€ training_history.npy
â”‚â”€â”€ simulate_big_data.py
â”‚â”€â”€ README.md
```

## ğŸš€ Steps Involved
### 1ï¸âƒ£ Generate Synthetic Data
- 1,000,000 rows with 10 random features.
- Stored in a Pandas DataFrame.
- Saved as a Parquet file for efficient storage.

### 2ï¸âƒ£ Preprocess the Data
- Split into **80% training** and **20% testing**.
- Standardized using `StandardScaler`.

### 3ï¸âƒ£ Build and Compile a Neural Network
- A simple feed-forward **autoencoder** with dropout layers.
- Compiled using the **Adam optimizer** and `mse` loss function.

### 4ï¸âƒ£ Train the Model
- Uses **early stopping** to prevent overfitting.
- Best model checkpoint saved as **best_model.keras**.

### 5ï¸âƒ£ Evaluate and Visualize Results
- Computes test **loss** and **mean absolute error (MAE)**.
- Plots **Training vs. Validation Loss**.
- Plots **Training vs. Validation MAE**.

## ğŸ“¦ Dependencies
Install required packages:
```bash
pip install numpy pandas scikit-learn tensorflow matplotlib pyarrow dask
```

## ğŸƒâ€â™‚ï¸ Run the Project
Execute the Python script:
```bash
python simulate_big_data.py
```

## ğŸ“Š Results
After training, visualizations will show:
- Loss curves over epochs.
- MAE curves over epochs.

## ğŸ”¹ Model Saving
The trained model is saved in the **Keras format (`best_model.keras`)** for future use:
```python
model.save("best_model.keras")
```

## ğŸ“ Notes
- This project demonstrates how to handle **big data simulation** and **deep learning workflows**.
- **Dask** can be used for parallel processing when scaling to larger datasets.

